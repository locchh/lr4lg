import numpy as np
from bert_score import score as bert_score
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge

def calculate_metrics(reference_texts, candidate_texts, relevant_indices):
    """
    Calculate BERTScore, ROUGE-L, BLEU-4, MAP, and F1-Score.

    :param reference_texts: List of reference sentences (ground truth).
    :param candidate_texts: List of candidate sentences (generated by the model).
    :param relevant_indices: List of lists, where each sublist contains indices of relevant candidates for each reference.
    :return: A dictionary with calculated metrics.
    """
    # Ensure the inputs are valid
    if len(reference_texts) != len(candidate_texts):
        raise ValueError("Reference and candidate lists must be of the same length.")
    
    if len(reference_texts) != len(relevant_indices):
        raise ValueError("Reference texts and relevant indices must be of the same length.")

    # Calculate BERTScore
    P, R, F1 = bert_score(candidate_texts, reference_texts, lang='en', return_hash=False)

    # Calculate ROUGE-L
    rouge = Rouge()
    rouge_scores = rouge.get_scores(candidate_texts, reference_texts, avg=True)

    # Calculate BLEU-4
    bleu_scores = [
        sentence_bleu([ref.split() for ref in reference_texts], candidate.split(), weights=(0.25, 0.25, 0.25, 0.25))
        for candidate in candidate_texts
    ]
    bleu_mean = np.mean(bleu_scores)

    # Calculate Mean Average Precision (MAP)
    average_precisions = []
    for indices in relevant_indices:
        if len(indices) == 0:
            average_precisions.append(0.0)
            continue
        correct = 0
        precisions = []
        for idx in range(len(candidate_texts)):
            if idx in indices:
                correct += 1
                precisions.append(correct / (idx + 1))
        average_precisions.append(np.mean(precisions) if precisions else 0.0)

    mean_ap = np.mean(average_precisions)

    # Calculate F1-Score
    f1_score = 2 * (P.mean() * R.mean()) / (P.mean() + R.mean() + 1e-10)  # Add a small value to avoid division by zero

    # Prepare results
    results = {
        'BERTScore': {
            'Precision': P.mean().item(),
            'Recall': R.mean().item(),
            'F1': F1.mean().item()
        },
        'ROUGE-L': {
            'F1': rouge_scores['rouge-l']['f'],
            'Precision': rouge_scores['rouge-l']['p'],
            'Recall': rouge_scores['rouge-l']['r']
        },
        'BLEU-4': bleu_mean,
        'MAP': mean_ap,
        'F1-Score': f1_score.item()  # Converting to a scalar
    }

    return results

# Example usage
reference_sentences = [
    "This is a test sentence.",
    "Here is another example of a reference."
]

candidate_sentences = [
    "This is a test.",
    "Here is another sample."
]

# Example relevant indices
relevant_indices = [
    [0],  # Assume first candidate is relevant for first reference
    []     # No relevant candidates for the second reference
]

metrics = calculate_metrics(reference_sentences, candidate_sentences, relevant_indices)
print(metrics)
